---
title: "Fitting Exercise"
---

Load the data into R

```{r}
library(here)
drug_mag = read.csv(here("fitting-exercise", "Mavoglurant_A2121_nmpk.csv"))
head(drug_mag)
```
Plot the data: 

```{r}
library(ggplot2)
ggplot(drug_mag, aes(x=TIME, y=DV, group=ID, color=DOSE)) +
  geom_line() +
  theme_minimal() +
  labs(title = "DV over time by dose",
       x = "Time",
       y = "DV") +
  theme(legend.title = element_blank())

```

Remove all entries where OCC=2
```{r}
library(dplyr)
drug_mag1 = drug_mag %>%
  filter(OCC==1)
```

Exclude observations with time = 0
```{r}
drug_mag2 = drug_mag1 %>%
  filter(TIME!= 0)
```

Compute the sum
```{r}
Y= drug_mag2 %>%
  group_by(ID) %>%
  summarize(Y = sum(DV, na.rm = TRUE))
print(Y)
```

Create a dataframe with time = 0
```{r}
drug_mag3 = drug_mag1 %>%
  filter(TIME == 0)
```

Join the data
```{r}
drug_mag_joined <- left_join(drug_mag3, Y, by = "ID")
```

Convert race and sex to factor variables, and retain necessary variables only.

```{r}
drug_mag_final = drug_mag_joined %>%
  mutate(
    RACE=factor(RACE),
    SEX=factor(SEX)) %>%
  select(ID, Y, DOSE, AGE, SEX, RACE, WT, HT)
```

Create a summary table:

```{r}
library(dplyr)
library(tidyr)
library(kableExtra)

# Summary stats for continuous variables
summary_stats <- drug_mag_final %>%
  summarize(
    Mean_Y = mean(Y, na.rm = TRUE),
    SD_Y = sd(Y, na.rm = TRUE),
    Mean_AGE = mean(AGE, na.rm = TRUE),
    SD_AGE = sd(AGE, na.rm = TRUE),
    Mean_WT = mean(WT, na.rm = TRUE),
    SD_WT = sd(WT, na.rm = TRUE),
    Mean_HT = mean(HT, na.rm = TRUE),
    SD_HT = sd(HT, na.rm = TRUE),
    Mean_Dose = mean(DOSE, na.rm = TRUE),
    SD_Dose = sd(DOSE, na.rm = TRUE)
  ) %>%
  pivot_longer(cols = everything(),
               names_to = c(".value", "Measure"),
               names_sep = "_") %>%
  mutate(
    Mean = format(round(Mean, 2), nsmall = 2),
    SD = format(round(SD, 2), nsmall = 2)
  ) %>%
  select(Measure, Mean, SD)

# Bigger blank row for separation
blank_row <- tibble(Measure = " ", Mean = " ", SD = " ")

# Subheading row for percentage-based variables
percent_heading <- tibble(Measure = "Categorical Variables", Mean = " ", SD = " ")

# Percentage for SEX
sex_summary <- drug_mag_final %>%
  count(SEX) %>%
  mutate(Percent = round(n / sum(n) * 100, 1)) %>%
  summarize(
    Measure = "SEX",
    Mean = paste0("SEX = 1: ", Percent[SEX == 1], "%"),
    SD = paste0("SEX = 2: ", Percent[SEX == 2], "%")
  )

# Percentage for RACE
race_summary <- drug_mag_final %>%
  count(RACE) %>%
  mutate(Percent = round(n / sum(n) * 100, 1)) %>%
  summarize(
    Measure = "RACE",
    Mean = paste0("RACE = 1: ", Percent[RACE == 1], "%"),
    SD = paste0("RACE = 2: ", Percent[RACE == 2], "%")
  )

# Combine all the tables
final_summary <- bind_rows(summary_stats, blank_row, blank_row, percent_heading, blank_row, sex_summary, race_summary)

# Display the final table with clear sectioning and subheadings
final_summary %>%
  kable("html", col.names = c("Measure", "Mean", "SD"),
        caption = "Summary Statistics and Percentages") %>%
  add_header_above(c(" " = 1, "Continuous Variables" = 2)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  row_spec(nrow(summary_stats) + 3, bold = TRUE, color = "black", hline_after = TRUE) %>% # Style the percentage-based subheading
  row_spec(nrow(summary_stats) + 1, hline_after = TRUE) # Visual break before percentages


```

We see that the mean age is 33.26. The mean height and weight are 83.60 and 1.76 - thus the mean BMI is 26.98 (overweight).


Scatterplots:

```{r}
library(ggplot2)

# Scatterplot of Y vs. AGE
ggplot(drug_mag_final, aes(x = AGE, y = Y)) +
  geom_point(color = 'blue') +
  labs(title = "Scatterplot of Y vs. AGE", x = "AGE", y = "Total Drug (Y)") +
  theme_minimal()

# Scatterplot of Y vs. WT (weight)
ggplot(drug_mag_final, aes(x = WT, y = Y)) +
  geom_point(color = 'red') +
  labs(title = "Scatterplot of Y vs. WT", x = "Weight (WT)", y = "Total Drug (Y)") +
  theme_minimal()

# Scatterplot of Y vs. HT (height)
ggplot(drug_mag_final, aes(x = HT, y = Y)) +
  geom_point(color = 'green') +
  labs(title = "Scatterplot of Y vs. HT", x = "Height (HT)", y = "Total Drug (Y)") +
  theme_minimal()

```

Age and total drug do not seem to have any linear relationship.
Similarly, no clear relationship is seem between total drug and weight or height.

Boxplots:

```{r}
# Boxplot of Y vs. SEX
ggplot(drug_mag_final, aes(x = factor(SEX), y = Y)) +
  geom_boxplot(fill = 'lightblue') +
  labs(title = "Boxplot of Y vs. SEX", x = "SEX", y = "Total Drug (Y)") +
  theme_minimal()

# Boxplot of Y vs. RACE
ggplot(drug_mag_final, aes(x = factor(RACE), y = Y)) +
  geom_boxplot(fill = 'lightgreen') +
  labs(title = "Boxplot of Y vs. RACE", x = "RACE", y = "Total Drug (Y)") +
  theme_minimal()

```
Sex = 2 has a slightly lower mean total drug administered. The mean total drug by race seems to be the same.

Distributions:
```{r}
library(ggplot2)
library(patchwork) # To combine plots

# Histogram for continuous variables
p1 <- ggplot(drug_mag_final, aes(x = Y)) +
  geom_histogram(fill = 'steelblue', color = 'black', bins = 30) +
  labs(title = "Distribution of Total Drug (Y)", x = "Total Drug (Y)", y = "Count") +
  theme_minimal()

p2 <- ggplot(drug_mag_final, aes(x = AGE)) +
  geom_histogram(fill = 'darkorange', color = 'black', bins = 30) +
  labs(title = "Distribution of Age", x = "Age", y = "Count") +
  theme_minimal()

p3 <- ggplot(drug_mag_final, aes(x = WT)) +
  geom_histogram(fill = 'forestgreen', color = 'black', bins = 30) +
  labs(title = "Distribution of Weight (WT)", x = "Weight", y = "Count") +
  theme_minimal()

p4 <- ggplot(drug_mag_final, aes(x = HT)) +
  geom_histogram(fill = 'purple', color = 'black', bins = 30) +
  labs(title = "Distribution of Height (HT)", x = "Height", y = "Count") +
  theme_minimal()

p5 <- ggplot(drug_mag_final, aes(x = DOSE)) +
  geom_histogram(fill = 'darkred', color = 'black', bins = 30) +
  labs(title = "Distribution of Dose", x = "Dose", y = "Count") +
  theme_minimal()

# Bar plots for categorical variables
p6 <- ggplot(drug_mag_final, aes(x = factor(SEX))) +
  geom_bar(fill = 'skyblue', color = 'black') +
  labs(title = "Distribution of Sex", x = "Sex", y = "Count") +
  theme_minimal()

p7 <- ggplot(drug_mag_final, aes(x = factor(RACE))) +
  geom_bar(fill = 'pink', color = 'black') +
  labs(title = "Distribution of Race", x = "Race", y = "Count") +
  theme_minimal()

# Combine the plots
(p1 + p2) / (p3 + p4) / (p5 + p6) / p7

```

The distributions are as expected. Weight is normally distributed. Height is slightly skewed. 
Age seems to have a bimodal distribution.

Correlation plots:
```{r}
library(GGally)
library(ggplot2)

# Select only continuous variables for pair plots
continuous_vars <- drug_mag_final %>%
  select(Y, AGE, WT, HT, DOSE)

# Pair plot with customized bin width
ggpairs(continuous_vars, 
        lower = list(continuous = wrap("points", alpha = 0.5)),
        upper = list(continuous = wrap("cor", size = 4)),
        diag = list(continuous = wrap("barDiag", fill = "steelblue", binwidth = 5)),
        title = "Pair and Correlation Plot of Continuous Variables")

```
The correlation between total drug and weight is significant (-0.210). The drug may have been dosed based on body weight. Other correlations are not relevant to the drug.


Model fitting:
```{r}
#Dose
dose_model = lm(Y~DOSE, data=drug_mag_final)
summary(dose_model)
```
We can see that dose is a significant predictor of Y. 
The R-squared is 0.5225, thus 52.25% of the variation in Y is explained by dose.


```{r}
#All predictors
full_model = lm(Y~ DOSE + HT + WT + AGE + as.factor(SEX) + as.factor(RACE), data=drug_mag_final)
summary(full_model)
```
We can see that dose and weight are significant predictors of Y.
The R-squared has increased to 0.6193, F-statistic = 22.57 and p-value < 0.05, thus this model is a better fit than the previous model.

Compare the root mean square error and R-squared:
```{r}
#Model 1
# Get predictions for model 1
predictions <- predict(dose_model)

# Calculate RMSE
actuals <- drug_mag_final$Y
rmse <- sqrt(mean((predictions - actuals)^2))

# Calculate R-squared
r_squared <- summary(dose_model)$r.squared

# Print results
cat("RMSE:", round(rmse, 2), "\n")
cat("R-squared:", round(r_squared, 2), "\n")


#Model 2
# Get predictions for model 2
predictions <- predict(full_model)

# Calculate RMSE
actuals <- drug_mag_final$Y
rmse <- sqrt(mean((predictions - actuals)^2))

# Calculate R-squared
r_squared <- summary(full_model)$r.squared

# Print results
cat("RMSE:", round(rmse, 2), "\n")
cat("R-squared:", round(r_squared, 2), "\n")


```
The RMSE for model 1 is greater than model 2, which means model 2 is a better fit.
Similarly, R-squared increased, thus model 2 explains 62% of the variation in Y.

Logistic regression:
Fitting a model to predict SEX using DOSE
```{r}
logistic_dose_model = glm(SEX~DOSE, data=drug_mag_final, family=binomial)
summary(logistic_dose_model)
```


We see that obviously, dose is not a significant predictor of sex.

Model with all predictors
```{r}
full_sex_model = glm(SEX~DOSE + Y + HT + WT + AGE + as.factor(RACE), data=drug_mag_final, family=binomial)
summary(full_sex_model)
```
Here, height is a significant predictor of sex. This makes sense since women are on average shorter than men. 

Accuracy and ROC-AUC:
```{r}
library(pROC)

# Model 1: Dose only
predicted_probs1 <- predict(logistic_dose_model, type = "response")
predicted_sex1 <- ifelse(predicted_probs1 > 0.5, 2, 1)

accuracy1 <- mean(predicted_sex1 == drug_mag_final$SEX)
cat("Model 1 - Accuracy:", round(accuracy1 * 100, 2), "%\n")

roc_curve1 <- roc(drug_mag_final$SEX, predicted_probs1)
auc_value1 <- auc(roc_curve1)
cat("Model 1 - ROC-AUC:", round(auc_value1, 3), "\n")

# Model 2: Full model (dose + other predictors)
predicted_probs2 <- predict(full_sex_model, type = "response")
predicted_sex2 <- ifelse(predicted_probs2 > 0.5, 2, 1)

accuracy2 <- mean(predicted_sex2 == drug_mag_final$SEX)
cat("Model 2 - Accuracy:", round(accuracy2 * 100, 2), "%\n")

roc_curve2 <- roc(drug_mag_final$SEX, predicted_probs2)
auc_value2 <- auc(roc_curve2)
cat("Model 2 - ROC-AUC:", round(auc_value2, 3), "\n")

```
We see that accuracy for Model 2 is greater than Model 1, thus model 2 is able to predict sex with greater accuracy (94.17% of the time).

Receiver operating characteristic - area under the curve (ROC-AUC) tells us how well the model is able to distinguish between SEX = 1 and SEX = 2.
For model 1, ROC-AUC = 0.592, so it is only slightly better at distinguishing compared to a random model.
For model 2, ROC-AUC = 0.98, thus it is excellent at distinguishing between the sexes.


Exercise 10: Model Improvement

Remove the variable race:
```{r}
drug_mag_final$RACE <- NULL
```

We now have a dataset with 120 observations and 6 variables.

Now, we set a random seed for reproducibility
```{r}
rngseed = 1234
```

Now, we split the data - 75% train and 25% test set:
```{r}
# Load the rsample package (part of tidymodels)
library(rsample)

# Set the seed for reproducibility
set.seed(rngseed)

# Split the data into 75% train and 25% test
data_split <- initial_split(drug_mag_final, prop = 3/4)

# Create data frames for the two sets
train_data <- training(data_split)  # 75% training set
test_data  <- testing(data_split)   # 25% test set
```

Now, we fit two models to the training data:
```{r}
library(tidymodels)

# Model 1: Only DOSE
dose_only_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression") %>%
  fit(Y ~ DOSE, data = train_data)

dose_only_predictions <- predict(dose_only_model, new_data = train_data) %>%
  bind_cols(train_data)

dose_only_rmse <- dose_only_predictions %>%
  rmse(truth = Y, estimate = .pred)

# Model 2: All predictors
full_predictors_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression") %>%
  fit(Y ~ DOSE + HT + WT + AGE + as.factor(SEX), data = train_data)

full_predictors_predictions <- predict(full_predictors_model, new_data = train_data) %>%
  bind_cols(train_data)

full_predictors_rmse <- full_predictors_predictions %>%
  rmse(truth = Y, estimate = .pred)

# Null Model
null_model <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("regression") %>%
  fit(Y ~ 1, data = train_data)

null_predictions <- predict(null_model, new_data = train_data) %>%
  bind_cols(train_data)

null_rmse <- null_predictions %>%
  rmse(truth = Y, estimate = .pred)

# Print all RMSE results in the desired order
cat("DOSE-only Model RMSE:", dose_only_rmse$.estimate, "\n")
cat("Full Predictors Model RMSE:", full_predictors_rmse$.estimate, "\n")
cat("Null Model RMSE:", null_rmse$.estimate, "\n")
```
We see that the highest RMSE value is observed with the null model, followed by the dose-only model, followed by the model using all the predictors. Since the model with all the predictors has the lowest RMSE, this model provides the best overall fit.

Now, we move on to the cross-validation part:
```{r}
# Set the random seed
set.seed(rngseed)

# Splitting the data into training and test sets
data_split <- initial_split(drug_mag_final, prop = 0.75)
train_data <- training(data_split)
test_data  <- testing(data_split)

```

Now, we perform the 10-fold cross-validation
```{r}
library(tidymodels)

# Set the random seed for reproducibility
set.seed(rngseed)

# Create a 10-fold cross-validation object
cv_folds <- vfold_cv(train_data, v = 10)

# Model 1: Dose as predictor
dose_only_cv_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Fit and evaluate Model 1 using 10-fold CV
dose_only_cv_results <- fit_resamples(
  dose_only_cv_model,
  Y ~ DOSE,
  resamples = cv_folds,
  metrics = metric_set(yardstick::rmse)  # Pass the metric function, not a string
)

# Extract the average RMSE across all folds
dose_only_cv_rmse <- dose_only_cv_results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  pull(mean)

cat("DOSE-only Model Average RMSE (10-fold CV):", dose_only_cv_rmse, "\n")


# Model 2: All predictors
# Specify the linear model
full_predictors_cv_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Fit and evaluate Model 2 using 10-fold CV
full_predictors_cv_results <- fit_resamples(
  full_predictors_cv_model,
  Y ~ DOSE + HT + WT + AGE + as.factor(SEX),
  resamples = cv_folds,
  metrics = metric_set(yardstick::rmse)  # Wrap rmse in metric_set()
)

# Extract the average RMSE across all folds
full_predictors_cv_rmse <- full_predictors_cv_results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  pull(mean)

cat("Full Predictors Model Average RMSE (10-fold CV):", full_predictors_cv_rmse, "\n")
```
The RMSE for the dose-only model decreased from 702 to 690, while that for the model with all the predictors increased from 627 to 645. 
Thus, using cross-validation improved the fit of one model.

Now, we compute standard error of the RMSE:
```{r}
# Extract RMSE for each fold for Model 1
dose_only_cv_rmse_folds <- dose_only_cv_results %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == "rmse")

# Compute average RMSE and standard error for Model 1
dose_only_cv_rmse <- mean(dose_only_cv_rmse_folds$.estimate)
dose_only_cv_rmse_se <- sd(dose_only_cv_rmse_folds$.estimate) / sqrt(nrow(dose_only_cv_rmse_folds))

# Extract RMSE for each fold for Model 2
full_predictors_cv_rmse_folds <- full_predictors_cv_results %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == "rmse")

# Compute average RMSE and standard error for Model 2
full_predictors_cv_rmse <- mean(full_predictors_cv_rmse_folds$.estimate)
full_predictors_cv_rmse_se <- sd(full_predictors_cv_rmse_folds$.estimate) / sqrt(nrow(full_predictors_cv_rmse_folds))

# Print results
cat("DOSE-only Model RMSE Standard Error:", dose_only_cv_rmse_se, "\n\n")
cat("Full Predictors Model RMSE Standard Error:", full_predictors_cv_rmse_se, "\n")


```
We can see that the RMSE standard error for the model with all the predictors is smaller, indicating that it is a more robust model compared to the dose-only model.

Now, we run the code again using a different random seed:
```{r}
library(tidymodels)  # Load tidymodels (includes rsample, parsnip, yardstick, etc.)

# Set a new random seed for reproducibility
set.seed(5678)  # Changed from rngseed = 1234 to 5678

# Create a 10-fold cross-validation object
cv_folds <- vfold_cv(train_data, v = 10)

# Define the models with new names
model_dose_only <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

model_full_predictors <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Fit and evaluate Model 1 (DOSE-only) using 10-fold CV
results_dose_only <- fit_resamples(
  model_dose_only,
  Y ~ DOSE,
  resamples = cv_folds,
  metrics = metric_set(yardstick::rmse)  # Use RMSE as the metric
)

# Fit and evaluate Model 2 (Full Predictors) using 10-fold CV
results_full_predictors <- fit_resamples(
  model_full_predictors,
  Y ~ DOSE + HT + WT + AGE + as.factor(SEX),
  resamples = cv_folds,
  metrics = metric_set(yardstick::rmse)  # Use RMSE as the metric
)

# Extract RMSE for each fold for Model 1 (DOSE-only)
rmse_folds_dose_only <- results_dose_only %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == "rmse")

# Compute average RMSE and standard error for Model 1 (DOSE-only)
avg_rmse_dose_only <- mean(rmse_folds_dose_only$.estimate)
se_rmse_dose_only <- sd(rmse_folds_dose_only$.estimate) / sqrt(nrow(rmse_folds_dose_only))

# Extract RMSE for each fold for Model 2 (Full Predictors)
rmse_folds_full_predictors <- results_full_predictors %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == "rmse")

# Compute average RMSE and standard error for Model 2 (Full Predictors)
avg_rmse_full_predictors <- mean(rmse_folds_full_predictors$.estimate)
se_rmse_full_predictors <- sd(rmse_folds_full_predictors$.estimate) / sqrt(nrow(rmse_folds_full_predictors))

# Print results
cat("DOSE-only Model Average RMSE (10-fold CV):", avg_rmse_dose_only, "\n")
cat("DOSE-only Model RMSE Standard Error:", se_rmse_dose_only, "\n\n")

cat("Full Predictors Model Average RMSE (10-fold CV):", avg_rmse_full_predictors, "\n")
cat("Full Predictors Model RMSE Standard Error:", se_rmse_full_predictors, "\n")
```
We can see that the RMSE did not change significantly - for the dose model, it increased from 690 to 693, while for the second model it increased from 645 to 649.
The standard errors decreased, model 2 is still a better fit than model 1.

# This section added by DOREEN KIBUULE KALEMBE.
These are our three models
1. Model with no predictors.
2. Model with only DOSE as predictor.
3. Model with ALL predictors.



```{r}
# Fitting the models
null_model <- lm(Y ~ 1, data = train_data) #model
dose_only_model <- lm(Y ~ DOSE, data = train_data) #model2
full_predictors_model <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data) #model3


# Generating predictions for each model
train_data$pred_null <- predict(null_model, newdata = train_data)
train_data$pred_model1 <- predict(dose_only_model, newdata = train_data)
train_data$pred_model2 <- predict(full_predictors_model, newdata = train_data)


# Creating a data frame with observed and predicted values for three models
results <- data.frame(
  Observed = train_data$Y,
  Predicted_NULL = train_data$pred_null,
  Predicted_DOSE = train_data$pred_model1,
  Predicted_ALL = train_data$pred_model2)
 
# printing the results
head(results) 
```
The predicted values of the null, dose and all predictors are gotten.

lets proceed and get some visualisations.
```{r}
#Reshape the data into pivot longer version for better ggplot plotting.
results_long <- results %>%
  pivot_longer(
    cols = starts_with("Predicted_"),
    names_to = "Model",
    values_to = "Predicted"
  )

ggplot(results_long, aes(x = Observed, y = Predicted, color = Model, shape = Model)) +
  geom_point(size = 3, alpha = 0.8) +  
  geom_abline(slope = 1, intercept = 0, linetype = "solid", color = "blue") +  # Fixed linetype
  facet_wrap(~ Model, ncol = 3) +  
  scale_x_continuous(limits = c(0, 5000)) +  
  scale_y_continuous(limits = c(0, 5000)) +  
  labs(
    x = "Observed Values",
    y = "Predicted Values",
    title = "Observed vs Predicted Values for Each Model",
    color = "Model",
    shape = "Model"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    strip.text = element_text(size = 10, face = "bold")
  )

```

From the above visualizations, we see that the model with all predictors shows a better fit to our data.


Now lets plot residuals vs predicted to see whether we have any patterns in the data of model two.
```{r}


# adding predictions and residuals to the data
results <- results %>% mutate(Model2_residuals = Predicted_ALL - Observed)

# creating the plot
ggplot(results, aes(x = Predicted_ALL, y = Model2_residuals)) +
  geom_point(size = 2, alpha = 0.6, color = "black") +  # adding points for predicted vs residuals
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Add horizontal line at 0
  labs(
    x = "Predicted Values (Model 2)",
    y = "Residuals (Predicted - Observed)",
    title = "Predicted vs Residuals for Model 2",
  ) +
  scale_y_continuous(limits = c(-max(abs(results$Model2_residuals)), max(abs(results$Model2_residuals)))) +  
  theme_bw() +
  theme(
    plot.title = element_text(size = 10, face = "bold"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )
```

From the above plot, we notice that we have more negative values than positive values and this could be a problem but we have nothing to do about it. 

```{r}
# Loading libraries
library(purrr)

# set the random seed back to rngseed
set.seed(1234)

# Create 100 bootstrap samples from the training data
bootstrap_samples <- bootstraps(train_data, times = 100)

# defining a function to fit Model 2 and make predictions
fit_and_predict <- function(split) {
  # saving the bootstrap sample
  bootstrap_data <- analysis(split)
  
  # fitting Model 2 (Y ~ DOSE + AGE + SEX + WT + HT) to the bootstrap sample
  model_2 <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = bootstrap_data)
  
  # making predictions for the original training data
  predict(model_2, newdata = train_data)
}

# using the function to each bootstrap sample and storing predictions
predictions_list <- map(bootstrap_samples$splits, fit_and_predict)

# converting the list of predictions to a matrix (array)
predictions_matrix <- do.call(rbind, predictions_list)

# calculate median and 95% confidence intervals for each data point
prediction_summary <- predictions_matrix %>%
  apply(2, quantile, probs = c(0.025, 0.5, 0.975)) %>%
  t() %>%
  as.data.frame() %>%
  setNames(c("Lower_CI_Median", "Pred_Median", "Upper_CI_Median"))
```


now lets calculate the mean and CIs.
```{r}
# calculating mean and 95% confidence intervals for each data point
mean_confint <- predictions_matrix %>%
  apply(2, function(x) {
    mean_val <- mean(x)
    se <- sd(x) / sqrt(length(x))  # standard error of the mean
    lower_ci <- mean_val - 1.96 * se  # 95% CI lower bound
    upper_ci <- mean_val + 1.96 * se  # 95% CI upper bound
    c(lower_ci, mean_val, upper_ci)
  }) %>%
  t()  %>%
  as.data.frame()  %>%
  setNames(c("Lower_CI_for_Mean", "Pred_Mean", "Upper_CI_for_Mean"))

# merging median and mean summaries
prediction_summary <- cbind(prediction_summary, mean_confint)

# adding the observed values from the training data
prediction_summary$Observed <- train_data$Y

# printing the updated summary
head(prediction_summary)
```
we have found the confidence interval for both the predicted median and mean.  

lets try to visualize it.
```{r}

# adding the original predictions to the summary table
prediction_summary$Original_Predictions <- predict(lm(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data), newdata = train_data)

# create the plot
ggplot(prediction_summary, aes(x = Observed)) +
  geom_point(aes(y = Original_Predictions), color = "darkgreen", size = 3.5, alpha = 0.8, shape = 16) +  # Original predictions (point estimate)
  geom_point(aes(y = Pred_Median), color = "black", size = 2, alpha = 0.7, shape = 10) +  # Median of bootstrap predictions (as points)
  geom_errorbar(aes(ymin = Lower_CI_Median, ymax = Upper_CI_Median), color = "orange", width = 0.2) +  # 95% CI for median
  geom_abline(slope = 1, intercept = 0, linetype = "solid", color = "red") +  # adding a 45-degree line
  labs(
    x = "Observed Values",
    y = "Predicted Values",
    title = "Observed values vs Predicted Values with Bootstrap CIs",
  ) +
  scale_x_continuous(limits = c(0, max(prediction_summary$Observed, prediction_summary$Upper_CI_Median))) +  # x-axis limits
  scale_y_continuous(limits = c(0, max(prediction_summary$Observed, prediction_summary$Upper_CI_Median))) +  #y-axis limits
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  )
```

Here, we see that Most predicted values increase as observed values increase.some points seem below the line at higher values which could mean that there was potential under estimation however,the predictions appear scattered around the red line, meaning the model does well overall and it is pretty accurate though there could be some variance.




