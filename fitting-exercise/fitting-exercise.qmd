---
title: "Fitting Exercise"
---

Load the data into R

```{r}
library(here)
drug_mag = read.csv(here("fitting-exercise", "Mavoglurant_A2121_nmpk.csv"))
head(drug_mag)
```
Plot the data: 

```{r}
library(ggplot2)
ggplot(drug_mag, aes(x=TIME, y=DV, group=ID, color=DOSE)) +
  geom_line() +
  theme_minimal() +
  labs(title = "DV over time by dose",
       x = "Time",
       y = "DV") +
  theme(legend.title = element_blank())

```

Remove all entries where OCC=2
```{r}
library(dplyr)
drug_mag1 = drug_mag %>%
  filter(OCC==1)
```

Exclude observations with time = 0
```{r}
drug_mag2 = drug_mag1 %>%
  filter(TIME!= 0)
```

Compute the sum
```{r}
Y= drug_mag2 %>%
  group_by(ID) %>%
  summarize(Y = sum(DV, na.rm = TRUE))
print(Y)
```

Create a dataframe with time = 0
```{r}
drug_mag3 = drug_mag1 %>%
  filter(TIME == 0)
```

Join the data
```{r}
drug_mag_joined <- left_join(drug_mag3, Y, by = "ID")
```

Convert race and sex to factor variables, and retain necessary variables only.

```{r}
drug_mag_final = drug_mag_joined %>%
  mutate(
    RACE=factor(RACE),
    SEX=factor(SEX)) %>%
  select(ID, Y, DOSE, AGE, SEX, RACE, WT, HT)
```

Save this as an RDS file:
```{r}
save_data_location <- here::here("fitting-exercise","drug_mag_final.rds")
saveRDS(drug_mag_final, file = save_data_location)
```

Create a summary table:

```{r}
library(dplyr)
library(tidyr)
library(kableExtra)

# Summary stats for continuous variables
summary_stats <- drug_mag_final %>%
  summarize(
    Mean_Y = mean(Y, na.rm = TRUE),
    SD_Y = sd(Y, na.rm = TRUE),
    Mean_AGE = mean(AGE, na.rm = TRUE),
    SD_AGE = sd(AGE, na.rm = TRUE),
    Mean_WT = mean(WT, na.rm = TRUE),
    SD_WT = sd(WT, na.rm = TRUE),
    Mean_HT = mean(HT, na.rm = TRUE),
    SD_HT = sd(HT, na.rm = TRUE),
    Mean_Dose = mean(DOSE, na.rm = TRUE),
    SD_Dose = sd(DOSE, na.rm = TRUE)
  ) %>%
  pivot_longer(cols = everything(),
               names_to = c(".value", "Measure"),
               names_sep = "_") %>%
  mutate(
    Mean = format(round(Mean, 2), nsmall = 2),
    SD = format(round(SD, 2), nsmall = 2)
  ) %>%
  select(Measure, Mean, SD)

# Bigger blank row for separation
blank_row <- tibble(Measure = " ", Mean = " ", SD = " ")

# Subheading row for percentage-based variables
percent_heading <- tibble(Measure = "Categorical Variables", Mean = " ", SD = " ")

# Percentage for SEX
sex_summary <- drug_mag_final %>%
  count(SEX) %>%
  mutate(Percent = round(n / sum(n) * 100, 1)) %>%
  summarize(
    Measure = "SEX",
    Mean = paste0("SEX = 1: ", Percent[SEX == 1], "%"),
    SD = paste0("SEX = 2: ", Percent[SEX == 2], "%")
  )

# Percentage for RACE
race_summary <- drug_mag_final %>%
  count(RACE) %>%
  mutate(Percent = round(n / sum(n) * 100, 1)) %>%
  summarize(
    Measure = "RACE",
    Mean = paste0("RACE = 1: ", Percent[RACE == 1], "%"),
    SD = paste0("RACE = 2: ", Percent[RACE == 2], "%")
  )

# Combine all the tables
final_summary <- bind_rows(summary_stats, blank_row, blank_row, percent_heading, blank_row, sex_summary, race_summary)

# Display the final table with clear sectioning and subheadings
final_summary %>%
  kable("html", col.names = c("Measure", "Mean", "SD"),
        caption = "Summary Statistics and Percentages") %>%
  add_header_above(c(" " = 1, "Continuous Variables" = 2)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) %>%
  row_spec(nrow(summary_stats) + 3, bold = TRUE, color = "black", hline_after = TRUE) %>% # Style the percentage-based subheading
  row_spec(nrow(summary_stats) + 1, hline_after = TRUE) # Visual break before percentages


```

We see that the mean age is 33.26. The mean height and weight are 83.60 and 1.76 - thus the mean BMI is 26.98 (overweight).


Scatterplots:

```{r}
library(ggplot2)

# Scatterplot of Y vs. AGE
ggplot(drug_mag_final, aes(x = AGE, y = Y)) +
  geom_point(color = 'blue') +
  labs(title = "Scatterplot of Y vs. AGE", x = "AGE", y = "Total Drug (Y)") +
  theme_minimal()

# Scatterplot of Y vs. WT (weight)
ggplot(drug_mag_final, aes(x = WT, y = Y)) +
  geom_point(color = 'red') +
  labs(title = "Scatterplot of Y vs. WT", x = "Weight (WT)", y = "Total Drug (Y)") +
  theme_minimal()

# Scatterplot of Y vs. HT (height)
ggplot(drug_mag_final, aes(x = HT, y = Y)) +
  geom_point(color = 'green') +
  labs(title = "Scatterplot of Y vs. HT", x = "Height (HT)", y = "Total Drug (Y)") +
  theme_minimal()

```

Age and total drug do not seem to have any linear relationship.
Similarly, no clear relationship is seem between total drug and weight or height.

Boxplots:

```{r}
# Boxplot of Y vs. SEX
ggplot(drug_mag_final, aes(x = factor(SEX), y = Y)) +
  geom_boxplot(fill = 'lightblue') +
  labs(title = "Boxplot of Y vs. SEX", x = "SEX", y = "Total Drug (Y)") +
  theme_minimal()

# Boxplot of Y vs. RACE
ggplot(drug_mag_final, aes(x = factor(RACE), y = Y)) +
  geom_boxplot(fill = 'lightgreen') +
  labs(title = "Boxplot of Y vs. RACE", x = "RACE", y = "Total Drug (Y)") +
  theme_minimal()

```
Sex = 2 has a slightly lower mean total drug administered. The mean total drug by race seems to be the same.

Distributions:
```{r}
library(ggplot2)
library(patchwork) # To combine plots

# Histogram for continuous variables
p1 <- ggplot(drug_mag_final, aes(x = Y)) +
  geom_histogram(fill = 'steelblue', color = 'black', bins = 30) +
  labs(title = "Distribution of Total Drug (Y)", x = "Total Drug (Y)", y = "Count") +
  theme_minimal()

p2 <- ggplot(drug_mag_final, aes(x = AGE)) +
  geom_histogram(fill = 'darkorange', color = 'black', bins = 30) +
  labs(title = "Distribution of Age", x = "Age", y = "Count") +
  theme_minimal()

p3 <- ggplot(drug_mag_final, aes(x = WT)) +
  geom_histogram(fill = 'forestgreen', color = 'black', bins = 30) +
  labs(title = "Distribution of Weight (WT)", x = "Weight", y = "Count") +
  theme_minimal()

p4 <- ggplot(drug_mag_final, aes(x = HT)) +
  geom_histogram(fill = 'purple', color = 'black', bins = 30) +
  labs(title = "Distribution of Height (HT)", x = "Height", y = "Count") +
  theme_minimal()

p5 <- ggplot(drug_mag_final, aes(x = DOSE)) +
  geom_histogram(fill = 'darkred', color = 'black', bins = 30) +
  labs(title = "Distribution of Dose", x = "Dose", y = "Count") +
  theme_minimal()

# Bar plots for categorical variables
p6 <- ggplot(drug_mag_final, aes(x = factor(SEX))) +
  geom_bar(fill = 'skyblue', color = 'black') +
  labs(title = "Distribution of Sex", x = "Sex", y = "Count") +
  theme_minimal()

p7 <- ggplot(drug_mag_final, aes(x = factor(RACE))) +
  geom_bar(fill = 'pink', color = 'black') +
  labs(title = "Distribution of Race", x = "Race", y = "Count") +
  theme_minimal()

# Combine the plots
(p1 + p2) / (p3 + p4) / (p5 + p6) / p7

```

The distributions are as expected. Weight is normally distributed. Height is slightly skewed. 
Age seems to have a bimodal distribution.

Correlation plots:
```{r}
library(GGally)
library(ggplot2)

# Select only continuous variables for pair plots
continuous_vars <- drug_mag_final %>%
  select(Y, AGE, WT, HT, DOSE)

# Pair plot with customized bin width
ggpairs(continuous_vars, 
        lower = list(continuous = wrap("points", alpha = 0.5)),
        upper = list(continuous = wrap("cor", size = 4)),
        diag = list(continuous = wrap("barDiag", fill = "steelblue", binwidth = 5)),
        title = "Pair and Correlation Plot of Continuous Variables")

```
The correlation between total drug and weight is significant (-0.210). The drug may have been dosed based on body weight. Other correlations are not relevant to the drug.


Model fitting:
```{r}
#Dose
dose_model = lm(Y~DOSE, data=drug_mag_final)
summary(dose_model)
```
We can see that dose is a significant predictor of Y. 
The R-squared is 0.5225, thus 52.25% of the variation in Y is explained by dose.


```{r}
#All predictors
full_model = lm(Y~ DOSE + HT + WT + AGE + as.factor(SEX) + as.factor(RACE), data=drug_mag_final)
summary(full_model)
```
We can see that dose and weight are significant predictors of Y.
The R-squared has increased to 0.6193, F-statistic = 22.57 and p-value < 0.05, thus this model is a better fit than the previous model.

Compare the root mean square error and R-squared:
```{r}
#Model 1
# Get predictions for model 1
predictions <- predict(dose_model)

# Calculate RMSE
actuals <- drug_mag_final$Y
rmse <- sqrt(mean((predictions - actuals)^2))

# Calculate R-squared
r_squared <- summary(dose_model)$r.squared

# Print results
cat("RMSE:", round(rmse, 2), "\n")
cat("R-squared:", round(r_squared, 2), "\n")


#Model 2
# Get predictions for model 2
predictions <- predict(full_model)

# Calculate RMSE
actuals <- drug_mag_final$Y
rmse <- sqrt(mean((predictions - actuals)^2))

# Calculate R-squared
r_squared <- summary(full_model)$r.squared

# Print results
cat("RMSE:", round(rmse, 2), "\n")
cat("R-squared:", round(r_squared, 2), "\n")


```
The RMSE for model 1 is greater than model 2, which means model 2 is a better fit.
Similarly, R-squared increased, thus model 2 explains 62% of the variation in Y.

Logistic regression:
Fitting a model to predict SEX using DOSE
```{r}
logistic_dose_model = glm(SEX~DOSE, data=drug_mag_final, family=binomial)
summary(logistic_dose_model)
```


We see that obviously, dose is not a significant predictor of sex.

Model with all predictors
```{r}
full_sex_model = glm(SEX~DOSE + Y + HT + WT + AGE + as.factor(RACE), data=drug_mag_final, family=binomial)
summary(full_sex_model)
```
Here, height is a significant predictor of sex. This makes sense since women are on average shorter than men. 

Accuracy and ROC-AUC:
```{r}
library(pROC)

# Model 1: Dose only
predicted_probs1 <- predict(logistic_dose_model, type = "response")
predicted_sex1 <- ifelse(predicted_probs1 > 0.5, 2, 1)

accuracy1 <- mean(predicted_sex1 == drug_mag_final$SEX)
cat("Model 1 - Accuracy:", round(accuracy1 * 100, 2), "%\n")

roc_curve1 <- roc(drug_mag_final$SEX, predicted_probs1)
auc_value1 <- auc(roc_curve1)
cat("Model 1 - ROC-AUC:", round(auc_value1, 3), "\n")

# Model 2: Full model (dose + other predictors)
predicted_probs2 <- predict(full_sex_model, type = "response")
predicted_sex2 <- ifelse(predicted_probs2 > 0.5, 2, 1)

accuracy2 <- mean(predicted_sex2 == drug_mag_final$SEX)
cat("Model 2 - Accuracy:", round(accuracy2 * 100, 2), "%\n")

roc_curve2 <- roc(drug_mag_final$SEX, predicted_probs2)
auc_value2 <- auc(roc_curve2)
cat("Model 2 - ROC-AUC:", round(auc_value2, 3), "\n")

```
We see that accuracy for Model 2 is greater than Model 1, thus model 2 is able to predict sex with greater accuracy (94.17% of the time).

Receiver operating characteristic - area under the curve (ROC-AUC) tells us how well the model is able to distinguish between SEX = 1 and SEX = 2.
For model 1, ROC-AUC = 0.592, so it is only slightly better at distinguishing compared to a random model.
For model 2, ROC-AUC = 0.98, thus it is excellent at distinguishing between the sexes.


Exercise 10: Model Improvement

Remove the variable race:
```{r}
drug_mag_final$RACE <- NULL
```

We now have a dataset with 120 observations and 6 variables.

Now, we set a random seed for reproducibility
```{r}
rngseed = 1234
```

Now, we split the data - 75% train and 25% test set:
```{r}
# Load the rsample package (part of tidymodels)
library(rsample)

# Set the seed for reproducibility
set.seed(rngseed)

# Split the data into 75% train and 25% test
data_split <- initial_split(drug_mag_final, prop = 3/4)

# Create data frames for the two sets
train_data <- training(data_split)  # 75% training set
test_data  <- testing(data_split)   # 25% test set
```

Now, we fit two models to the training data:
```{r}
library(tidymodels)

# Model 1: Only DOSE
dose_only_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression") %>%
  fit(Y ~ DOSE, data = train_data)

dose_only_predictions <- predict(dose_only_model, new_data = train_data) %>%
  bind_cols(train_data)

dose_only_rmse <- dose_only_predictions %>%
  rmse(truth = Y, estimate = .pred)

# Model 2: All predictors
full_predictors_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression") %>%
  fit(Y ~ DOSE + HT + WT + AGE + as.factor(SEX), data = train_data)

full_predictors_predictions <- predict(full_predictors_model, new_data = train_data) %>%
  bind_cols(train_data)

full_predictors_rmse <- full_predictors_predictions %>%
  rmse(truth = Y, estimate = .pred)

# Null Model
null_model <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("regression") %>%
  fit(Y ~ 1, data = train_data)

null_predictions <- predict(null_model, new_data = train_data) %>%
  bind_cols(train_data)

null_rmse <- null_predictions %>%
  rmse(truth = Y, estimate = .pred)

# Print all RMSE results in the desired order
cat("DOSE-only Model RMSE:", dose_only_rmse$.estimate, "\n")
cat("Full Predictors Model RMSE:", full_predictors_rmse$.estimate, "\n")
cat("Null Model RMSE:", null_rmse$.estimate, "\n")
```
We see that the highest RMSE value is observed with the null model, followed by the dose-only model, followed by the model using all the predictors. Since the model with all the predictors has the lowest RMSE, this model provides the best overall fit.

Now, we move on to the cross-validation part:
```{r}
# Set the random seed
set.seed(rngseed)

# Splitting the data into training and test sets
data_split <- initial_split(drug_mag_final, prop = 0.75)
train_data <- training(data_split)
test_data  <- testing(data_split)

```

Now, we perform the 10-fold cross-validation
```{r}
library(tidymodels)

# Set the random seed for reproducibility
set.seed(rngseed)

# Create a 10-fold cross-validation object
cv_folds <- vfold_cv(train_data, v = 10)

# Model 1: Dose as predictor
dose_only_cv_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Fit and evaluate Model 1 using 10-fold CV
dose_only_cv_results <- fit_resamples(
  dose_only_cv_model,
  Y ~ DOSE,
  resamples = cv_folds,
  metrics = metric_set(yardstick::rmse)  # Pass the metric function, not a string
)

# Extract the average RMSE across all folds
dose_only_cv_rmse <- dose_only_cv_results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  pull(mean)

cat("DOSE-only Model Average RMSE (10-fold CV):", dose_only_cv_rmse, "\n")


# Model 2: All predictors
# Specify the linear model
full_predictors_cv_model <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Fit and evaluate Model 2 using 10-fold CV
full_predictors_cv_results <- fit_resamples(
  full_predictors_cv_model,
  Y ~ DOSE + HT + WT + AGE + as.factor(SEX),
  resamples = cv_folds,
  metrics = metric_set(yardstick::rmse)  # Wrap rmse in metric_set()
)

# Extract the average RMSE across all folds
full_predictors_cv_rmse <- full_predictors_cv_results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  pull(mean)

cat("Full Predictors Model Average RMSE (10-fold CV):", full_predictors_cv_rmse, "\n")
```
The RMSE for the dose-only model decreased from 702 to 690, while that for the model with all the predictors increased from 627 to 645. 
Thus, using cross-validation improved the fit of one model.

Now, we compute standard error of the RMSE:
```{r}
# Extract RMSE for each fold for Model 1
dose_only_cv_rmse_folds <- dose_only_cv_results %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == "rmse")

# Compute average RMSE and standard error for Model 1
dose_only_cv_rmse <- mean(dose_only_cv_rmse_folds$.estimate)
dose_only_cv_rmse_se <- sd(dose_only_cv_rmse_folds$.estimate) / sqrt(nrow(dose_only_cv_rmse_folds))

# Extract RMSE for each fold for Model 2
full_predictors_cv_rmse_folds <- full_predictors_cv_results %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == "rmse")

# Compute average RMSE and standard error for Model 2
full_predictors_cv_rmse <- mean(full_predictors_cv_rmse_folds$.estimate)
full_predictors_cv_rmse_se <- sd(full_predictors_cv_rmse_folds$.estimate) / sqrt(nrow(full_predictors_cv_rmse_folds))

# Print results
cat("DOSE-only Model RMSE Standard Error:", dose_only_cv_rmse_se, "\n\n")
cat("Full Predictors Model RMSE Standard Error:", full_predictors_cv_rmse_se, "\n")


```
We can see that the RMSE standard error for the model with all the predictors is smaller, indicating that it is a more robust model compared to the dose-only model.

Now, we run the code again using a different random seed:
```{r}
library(tidymodels)  # Load tidymodels (includes rsample, parsnip, yardstick, etc.)

# Set a new random seed for reproducibility
set.seed(5678)  # Changed from rngseed = 1234 to 5678

# Create a 10-fold cross-validation object
cv_folds <- vfold_cv(train_data, v = 10)

# Define the models with new names
model_dose_only <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

model_full_predictors <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# Fit and evaluate Model 1 (DOSE-only) using 10-fold CV
results_dose_only <- fit_resamples(
  model_dose_only,
  Y ~ DOSE,
  resamples = cv_folds,
  metrics = metric_set(yardstick::rmse)  # Use RMSE as the metric
)

# Fit and evaluate Model 2 (Full Predictors) using 10-fold CV
results_full_predictors <- fit_resamples(
  model_full_predictors,
  Y ~ DOSE + HT + WT + AGE + as.factor(SEX),
  resamples = cv_folds,
  metrics = metric_set(yardstick::rmse)  # Use RMSE as the metric
)

# Extract RMSE for each fold for Model 1 (DOSE-only)
rmse_folds_dose_only <- results_dose_only %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == "rmse")

# Compute average RMSE and standard error for Model 1 (DOSE-only)
avg_rmse_dose_only <- mean(rmse_folds_dose_only$.estimate)
se_rmse_dose_only <- sd(rmse_folds_dose_only$.estimate) / sqrt(nrow(rmse_folds_dose_only))

# Extract RMSE for each fold for Model 2 (Full Predictors)
rmse_folds_full_predictors <- results_full_predictors %>%
  collect_metrics(summarize = FALSE) %>%
  filter(.metric == "rmse")

# Compute average RMSE and standard error for Model 2 (Full Predictors)
avg_rmse_full_predictors <- mean(rmse_folds_full_predictors$.estimate)
se_rmse_full_predictors <- sd(rmse_folds_full_predictors$.estimate) / sqrt(nrow(rmse_folds_full_predictors))

# Print results
cat("DOSE-only Model Average RMSE (10-fold CV):", avg_rmse_dose_only, "\n")
cat("DOSE-only Model RMSE Standard Error:", se_rmse_dose_only, "\n\n")

cat("Full Predictors Model Average RMSE (10-fold CV):", avg_rmse_full_predictors, "\n")
cat("Full Predictors Model RMSE Standard Error:", se_rmse_full_predictors, "\n")
```
We can see that the RMSE did not change significantly - for the dose model, it increased from 690 to 693, while for the second model it increased from 645 to 649.
The standard errors decreased, model 2 is still a better fit than model 1.

# This section added by DOREEN KIBUULE KALEMBE.
These are our three models
1. Model with no predictors.
2. Model with only DOSE as predictor.
3. Model with ALL predictors.

```{r}
# Fitting the models
null_model <- lm(Y ~ 1, data = train_data) #model
dose_only_model <- lm(Y ~ DOSE, data = train_data) #model2
full_predictors_model <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data) #model3


# Generating predictions for each model
train_data$pred_null <- predict(null_model, newdata = train_data)
train_data$pred_model1 <- predict(dose_only_model, newdata = train_data)
train_data$pred_model2 <- predict(full_predictors_model, newdata = train_data)


# Creating a data frame with observed and predicted values for three models
results <- data.frame(
  Observed = train_data$Y,
  Predicted_NULL = train_data$pred_null,
  Predicted_DOSE = train_data$pred_model1,
  Predicted_ALL = train_data$pred_model2)
 
# printing the results
head(results) 
```
The predicted values of the null, dose and all predictors are gotten.

lets proceed and get some visualisations.
```{r}
#Reshape the data into pivot longer version for better ggplot plotting.
results_long <- results %>%
  pivot_longer(
    cols = starts_with("Predicted_"),
    names_to = "Model",
    values_to = "Predicted"
  )

ggplot(results_long, aes(x = Observed, y = Predicted, color = Model, shape = Model)) +
  geom_point(size = 3, alpha = 0.8) +  
  geom_abline(slope = 1, intercept = 0, linetype = "solid", color = "blue") +  # Fixed linetype
  facet_wrap(~ Model, ncol = 3) +  
  scale_x_continuous(limits = c(0, 5000)) +  
  scale_y_continuous(limits = c(0, 5000)) +  
  labs(
    x = "Observed Values",
    y = "Predicted Values",
    title = "Observed vs Predicted Values for Each Model",
    color = "Model",
    shape = "Model"
  ) +
  theme_bw() +
  theme(
    legend.position = "bottom",
    strip.text = element_text(size = 10, face = "bold")
  )

```

From the above visualizations, we see that the model with all predictors shows a better fit to our data.


Now lets plot residuals vs predicted to see whether we have any patterns in the data of model two.
```{r}


# adding predictions and residuals to the data
results <- results %>% mutate(Model2_residuals = Predicted_ALL - Observed)

# creating the plot
ggplot(results, aes(x = Predicted_ALL, y = Model2_residuals)) +
  geom_point(size = 2, alpha = 0.6, color = "black") +  # adding points for predicted vs residuals
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +  # Add horizontal line at 0
  labs(
    x = "Predicted Values (Model 2)",
    y = "Residuals (Predicted - Observed)",
    title = "Predicted vs Residuals for Model 2",
  ) +
  scale_y_continuous(limits = c(-max(abs(results$Model2_residuals)), max(abs(results$Model2_residuals)))) +  
  theme_bw() +
  theme(
    plot.title = element_text(size = 10, face = "bold"),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10)
  )
```

From the above plot, we notice that we have more negative values than positive values and this could be a problem but we have nothing to do about it. 

```{r}
# Loading libraries
library(purrr)

# set the random seed back to rngseed
set.seed(1234)

# Create 100 bootstrap samples from the training data
bootstrap_samples <- bootstraps(train_data, times = 100)

# defining a function to fit Model 2 and make predictions
fit_and_predict <- function(split) {
  # saving the bootstrap sample
  bootstrap_data <- analysis(split)
  
  # fitting Model 2 (Y ~ DOSE + AGE + SEX + WT + HT) to the bootstrap sample
  model_2 <- lm(Y ~ DOSE + AGE + SEX + WT + HT, data = bootstrap_data)
  
  # making predictions for the original training data
  predict(model_2, newdata = train_data)
}

# using the function to each bootstrap sample and storing predictions
predictions_list <- map(bootstrap_samples$splits, fit_and_predict)

# converting the list of predictions to a matrix (array)
predictions_matrix <- do.call(rbind, predictions_list)

# calculate median and 95% confidence intervals for each data point
prediction_summary <- predictions_matrix %>%
  apply(2, quantile, probs = c(0.025, 0.5, 0.975)) %>%
  t() %>%
  as.data.frame() %>%
  setNames(c("Lower_CI_Median", "Pred_Median", "Upper_CI_Median"))
```


now lets calculate the mean and CIs.
```{r}
# calculating mean and 95% confidence intervals for each data point
mean_confint <- predictions_matrix %>%
  apply(2, function(x) {
    mean_val <- mean(x)
    se <- sd(x) / sqrt(length(x))  # standard error of the mean
    lower_ci <- mean_val - 1.96 * se  # 95% CI lower bound
    upper_ci <- mean_val + 1.96 * se  # 95% CI upper bound
    c(lower_ci, mean_val, upper_ci)
  }) %>%
  t()  %>%
  as.data.frame()  %>%
  setNames(c("Lower_CI_for_Mean", "Pred_Mean", "Upper_CI_for_Mean"))

# merging median and mean summaries
prediction_summary <- cbind(prediction_summary, mean_confint)

# adding the observed values from the training data
prediction_summary$Observed <- train_data$Y

# printing the updated summary
head(prediction_summary)
```
we have found the confidence interval for both the predicted median and mean.  

lets try to visualize it.
```{r}

# adding the original predictions to the summary table
prediction_summary$Original_Predictions <- predict(lm(Y ~ DOSE + AGE + SEX + WT + HT, data = train_data), newdata = train_data)

# create the plot
ggplot(prediction_summary, aes(x = Observed)) +
  geom_point(aes(y = Original_Predictions), color = "darkgreen", size = 3.5, alpha = 0.8, shape = 16) +  # Original predictions (point estimate)
  geom_point(aes(y = Pred_Median), color = "black", size = 2, alpha = 0.7, shape = 10) +  # Median of bootstrap predictions (as points)
  geom_errorbar(aes(ymin = Lower_CI_Median, ymax = Upper_CI_Median), color = "orange", width = 0.2) +  # 95% CI for median
  geom_abline(slope = 1, intercept = 0, linetype = "solid", color = "red") +  # adding a 45-degree line
  labs(
    x = "Observed Values",
    y = "Predicted Values",
    title = "Observed values vs Predicted Values with Bootstrap CIs",
  ) +
  scale_x_continuous(limits = c(0, max(prediction_summary$Observed, prediction_summary$Upper_CI_Median))) +  # x-axis limits
  scale_y_continuous(limits = c(0, max(prediction_summary$Observed, prediction_summary$Upper_CI_Median))) +  #y-axis limits
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 14),
    axis.text = element_text(size = 12)
  )
```

Here, we see that Most predicted values increase as observed values increase.some points seem below the line at higher values which could mean that there was potential under estimation however,the predictions appear scattered around the red line, meaning the model does well overall and it is pretty accurate though there could be some variance.


# Part 3 - Pooja Gokhale
We can see that the model with all the predictors has the best performance. It is scattered around the 45 degree line, with not many large deviations or outliers. Using cross-validation, we also found that this model has the lowest RMSE, and thus the best overall fit of the three models. 
Using bootstrapping, the results were similar, and the 95% CI are narrow, indicating low degree of uncertainty.
Since the results with bootstarpping are similar to the point estimate, there is limited variability due to sampling and the model is robust.
In comparison, the model with only dose as predictor does not perform as well. The null model is the most basic model and does not perform well.

Model evaluation using test data:

```{r}
# Make predictions on the test data
test_data$pred_model2 <- predict(full_predictors_model, newdata = test_data)

# Create the new data frame
combined_data <- data.frame(
  Observed = train_data$Y,  # Column 1: Observed values from training data
  Pred_Train = train_data$pred_model2,  # Column 2: Predictions from training data
  Pred_Test = test_data$pred_model2  # Column 3: Predictions from test data
)

# View the combined data frame
print(combined_data)

```
Now, we plot predictions for both train and test data versus the observed values

```{r}
# Load necessary library
library(ggplot2)

# Create the plot
ggplot(combined_data) +
  geom_point(aes(x = Observed, y = Pred_Train, color = "Training Predictions"), size = 3, alpha = 0.7) +  # Training data points
  geom_point(aes(x = Observed, y = Pred_Test, color = "Test Predictions"), size = 3, alpha = 0.7) +  # Test data points
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "black") +  # 1:1 reference line
  labs(
    title = "Predicted vs Observed",
    x = "Observed Values",
    y = "Predicted Values",
    color = "Dataset"
  ) +
  theme_minimal() +
  scale_color_manual(values = c("Training Predictions" = "blue", "Test Predictions" = "red")) +  # Custom colors
  coord_fixed(ratio = 1, xlim = range(combined_data$Observed, na.rm = TRUE), ylim = range(combined_data$Observed, na.rm = TRUE))  # Equal axis limits
```

We see that train and test data is mixed in.
In general, the model seems to perform better on train data than on test data.

Overall assessment:
Firs, let's look at model summaries
```{r}
summary(dose_only_model)
summary(full_predictors_model)
```


1. Our model with all the predictors performs better than the null (mean) model. The fit for this model is better than the null model, as evidenced by the plots and RMSE values. The model states that the dose is significantly associated with Y (conc.), which makes sense.

2. Model 1 with only dose is better than the null model. The RMSE is lower, and the graph shows a better fit. This model is probably not usable for any real purpose, since there are other predictors than may influence Y.

3. The model 2 with all the predictors further improves results. Model 2 results indicate that dose, sex and weight are significantly associated with Y (conc.). This also makes sense as dose determines the concentration of the drug, and dose depends upon weight. Females generally have a lower body weight, and thus sex being significant also makes sense. As for the usability of this model, I am not sure. The model does have a good overall fit, however, the sample size is quite small (90 for training data, 120 for the complete data). Additionally, we did not consider other model metrics, R-squared. We also did not use any variable selection methods, and while the model performed okay on the test data, it was not as good as the training data. More data would probably be required to make this model more robust.
